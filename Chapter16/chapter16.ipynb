{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用递归神经网络对序列数据建模"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 介绍序列数据\n",
    "- 用于序列建模的RNN\n",
    "- 长短期记忆LSTM\n",
    "- 延时间截断反向传播(T-BPTT)\n",
    "- 在TensorFlow实现一个用于序列建模的多层RNN\n",
    "- 项目1 - 用RNN对IMDb电影评论数据集进行情感分析\n",
    "- 项目2 - 使用来自莎士比亚《哈姆雷特》的文本数据，使用LSTM单元进行RNN字符级语言建模\n",
    "- 使用梯度削波，以避免爆炸的梯度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 介绍序列数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 序列数据建模 - 顺序关系"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 表示序列"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![1](1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN和CNN,MLP不同的地方在于:\n",
    "\n",
    "RNN具有记忆过去信息的能力并对新数据进行相应的处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 不同种类的序列建模"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![2](2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __Many-to-one__: 输入是一个序列但是输出是一个固定大小的向量,例如情感分析,输入是文本,输出是类标签\n",
    "- __One-to-many__: 输入是一个标准形式但是输出是序列,例如图像描述,输入是图片,输入是一个英文短语\n",
    "- __Many-to-many__: 输入输出都是序列,这个类别更进一步为基于输入输出是否同步,若同步,例如视频分类(视频每一帧都是有标签的).若不同步则比如将一种语言翻译成另一种语言"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 用于序列建模的RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 理解RNN的结构和流"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![3](3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![4](4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 在RNN计算激活项"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $W_{xh}$: 输入层和隐藏层之间的权重矩阵\n",
    "- $W_{hh}$: 递归边缘相关联的权重矩阵\n",
    "- $W_{hy}$: 隐藏层和输出层之间的权重矩阵\n",
    "\n",
    "![5](5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "net输入<br>\n",
    "$z_h^{(t)} = W_{xh}x^{(t)} + W_{hh}h^{(h-1)} + b_n$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "隐藏层的激活项为\n",
    "\\begin{equation}\n",
    "\\boldsymbol{h}^{(t)}=\\phi_{h}\\left(z_{h}^{(t)}\\right)=\\phi_{h}\\left(\\boldsymbol{W}_{x h} \\boldsymbol{x}^{(t)}+\\boldsymbol{W}_{h h} \\boldsymbol{h}^{(t-1)}+\\boldsymbol{b}_{h}\\right)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\boldsymbol{h}^{(t)}=\\phi_{h}\\left(\\left[\\boldsymbol{W}_{x h} ; \\boldsymbol{W}_{h h}\\right]\\left[\\begin{array}{c}{\\boldsymbol{x}^{(t)}} \\\\ {\\boldsymbol{h}^{(t-1)}}\\end{array}\\right]+\\boldsymbol{b}_{h}\\right)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\boldsymbol{y}^{(t)}=\\phi_{y}\\left(\\boldsymbol{W}_{h y} \\boldsymbol{h}^{(t)}+\\boldsymbol{b}_{y}\\right)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![6](6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 长期交互学习的挑战"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所谓的vanishing或者exploding梯度问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![7](7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "two solutions:\n",
    "- TBPTT\n",
    "- LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM单元"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![8](8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\odot$ refers to the element-wise product (element-wise multiplication) \n",
    "and $\\oplus$ means element-wise summation (element-wise addition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- forge gate($f_t$) 允许记忆单元重置细胞状态而不会无限期增长\n",
    "$$\n",
    "f_t=\\sigma\\left(W_x f x^{(t)}+W_{h f} h^{(t-1)}+b_{f}\\right)\n",
    "$$\n",
    "- input gate($i_t$)和input node($g_t$)用于更新细胞状态\n",
    "$$\n",
    "i_{t}=\\sigma\\left(W_{x i} x^{(t)}+W_{h i} h^{(t-1)}+b_{i}\\right)\n",
    "$$\n",
    "$$\n",
    "g_{t}=\\tanh \\left(W_{x g} x^{(t)}+W_{h g} h^{(t-1)}+b_{g}\\right)\n",
    "$$\n",
    "$$\n",
    "C^{(t)}=\\left(C^{(t-1)} \\odot f_{t}\\right) \\oplus\\left(i_{t} \\odot g_{t}\\right)\n",
    "$$\n",
    "- output gate($o_T$)决定隐藏层单元值的更新\n",
    "$$\n",
    "o_{t}=\\sigma\\left(W_{x o} x^{(t)}+W_{h o} h^{(t-1)}+b_{o}\\right)\n",
    "$$\n",
    "$$\n",
    "h_{(t)}=o_t \\odot \\tanh(C^{(t)})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 在TensorFlow实现一个用于序列建模的多层RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "two common problems tasks:\n",
    "- Sentiment analysis\n",
    "- Language modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 项目1 - 用RNN对IMDb电影评论数据集进行情感分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 准备数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyprind\n",
    "import pandas as pd\n",
    "from string import punctuation\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('movie_data.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In 1974, the teenager Martha Moxley (Maggie Gr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OK... so... I really like Kris Kristofferson a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>***SPOILER*** Do not read this, if you think a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hi for all the people who have seen this wonde...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I recently bought the DVD, forgetting just how...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  In 1974, the teenager Martha Moxley (Maggie Gr...          1\n",
       "1  OK... so... I really like Kris Kristofferson a...          0\n",
       "2  ***SPOILER*** Do not read this, if you think a...          0\n",
       "3  hi for all the people who have seen this wonde...          1\n",
       "4  I recently bought the DVD, forgetting just how...          0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Counting words occurences\n",
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:02:23\n",
      "Map reviews to int\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', '.', ',', 'and', 'a']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:02\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing the data:\n",
    "# separate words and\n",
    "# count each word's occurence\n",
    "from collections import Counter\n",
    "\n",
    "counts = Counter()\n",
    "pbar = pyprind.ProgBar(len(df['review']), title='Counting words occurences')\n",
    "\n",
    "for i, review in enumerate(df['review']):\n",
    "    text = ''.join([c if c not in punctuation else ' ' +\n",
    "                    c + ' ' for c in review]).lower()\n",
    "    df.loc[i, 'review'] = text\n",
    "    pbar.update()\n",
    "    counts.update(text.split())\n",
    "\n",
    "# create a mapping\n",
    "# map each unique word to a integer\n",
    "word_counts = sorted(counts, key=counts.get, reverse=True)\n",
    "print(word_counts[:5])\n",
    "word_to_int = {word: ii for ii, word in enumerate(word_counts, 1)}\n",
    "\n",
    "mapped_reviews = []\n",
    "pbar = pyprind.ProgBar(len(df['review']), title='Map reviews to int')\n",
    "\n",
    "for review in df['review']:\n",
    "    mapped_reviews.append([word_to_int[word] for word in review.split()])\n",
    "    pbar.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 为了生成匹配RNN架构的输入数据,我们需要保证所有的sequences有相同的长度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![9](9.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 持久化 mapped_reviews\n",
    "import json\n",
    "\n",
    "with open('mapped_reviews.txt', 'w') as f:\n",
    "    f.write(json.dumps(mapped_reviews))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('mapped_reviews.txt', 'r') as f:\n",
    "    mapped_reviews = json.loads(f.read())\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequence_length是一个可以被调优的超参\n",
    "\n",
    "# Define same-length sequences\n",
    "# if sequence length < 200: left_pad with zero\n",
    "# if sequence length > 200: use the last 200 elements\n",
    "sequence_length = 200\n",
    "sequences = np.zeros((len(mapped_reviews), sequence_length), dtype=int)\n",
    "\n",
    "for i,row in enumerate(mapped_reviews):\n",
    "    review_arr = np.array(row)\n",
    "    sequences[i,-len(row):] = review_arr[-sequence_length:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = sequences[:25000, :]\n",
    "y_train = df.loc[:25000, 'sentiment'].values\n",
    "X_test = sequences[25000:, :]\n",
    "y_test = df.loc[25000:, 'sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mini batch\n",
    "np.random.seed(123)\n",
    "\n",
    "# define a function to generate mini-batches\n",
    "\n",
    "\n",
    "def create_batch_generator(x, y=None, batch_size=64):\n",
    "    n_batches = len(x)//batch_size\n",
    "    x = x[:n_batches*batch_size]\n",
    "    if y is not None:\n",
    "        y = y[:n_batches*batch_size]\n",
    "    for ii in range(0, len(x), batch_size):\n",
    "        if y is not None:\n",
    "            yield x[ii:ii+batch_size], y[ii:ii+batch_size]\n",
    "        else:\n",
    "            yield x[ii:ii+batch_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "embedding相比于one-hot的优点:\n",
    "- 减小了特征空间的维数,降低了维数诅咒的效果\n",
    "- 神经网络在embedding层进行主要特征的提取的过程是可训练的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![10](10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建一个embedding层需要两步:\n",
    "- 创建一个$[n\\_words \\times embedding\\_size]$的tensor,并用[-1,1]之间的随机浮点数来初始化它\n",
    "```python\n",
    "embedding = tf.Variable(tf.random_uniform(shape=(n_words, embedding_size),\n",
    "                                         minval=-1, maxval=1))\n",
    "```\n",
    "- 使用`tf.nn.embedding_lookup`函数查找`tf_x`中每一个元素在embedding矩阵所关联的行\n",
    "```\n",
    "embed_x = tf.nn.embedding_lookup(embedding, tf_x)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 建立一个RNN模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SentimentRNN\n",
    "- constructor\n",
    "- build method\n",
    "- train method\n",
    "- predict method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class SentimentRNN(object):\n",
    "    def __init__(self, n_words, seq_len=200,\n",
    "                 lstm_size=256, num_layers=1, batch_size=64,\n",
    "                 learning_rate=0.0001, embed_size=200):\n",
    "        self.n_words = n_words\n",
    "        self.seq_len = seq_len\n",
    "        self.lstm_size = lstm_size  # number of hidden units\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.embed_size = embed_size\n",
    "\n",
    "        self.g = tf.Graph()\n",
    "        with self.g.as_default():\n",
    "            tf.set_random_seed(123)\n",
    "            self.build()\n",
    "            self.saver = tf.train.Saver()\n",
    "            self.init_op = tf.global_variables_initializer()\n",
    "\n",
    "    def build(self):\n",
    "        # Define the placeholders\n",
    "        tf_x = tf.placeholder(tf.int32,\n",
    "                              shape=(self.batch_size, self.seq_len),\n",
    "                              name='tf_x')\n",
    "        tf_y = tf.placeholder(tf.float32,\n",
    "                              shape=(self.batch_size),\n",
    "                              name='tf_y')\n",
    "        tf_keepprob = tf.placeholder(tf.float32,\n",
    "                                     name='tf_keepprob')\n",
    "        # Create the embedding layer\n",
    "        embedding = tf.Variable(\n",
    "            tf.random_uniform(\n",
    "                (self.n_words, self.embed_size),\n",
    "                minval=-1, maxval=1),\n",
    "            name='embedding')\n",
    "        embed_x = tf.nn.embedding_lookup(\n",
    "            embedding, tf_x,\n",
    "            name='embeded_x')\n",
    "\n",
    "        # Define LSTM cell and stack them together\n",
    "        cells = tf.contrib.rnn.MultiRNNCell(\n",
    "            [tf.contrib.rnn.DropoutWrapper(\n",
    "                tf.contrib.rnn.BasicLSTMCell(self.lstm_size),\n",
    "                output_keep_prob=tf_keepprob)\n",
    "             for i in range(self.num_layers)])\n",
    "\n",
    "        # Define the initial state:\n",
    "        self.initial_state = cells.zero_state(\n",
    "            self.batch_size, tf.float32)\n",
    "        print('  << initial state >> ', self.initial_state)\n",
    "\n",
    "        lstm_outputs, self.final_state = tf.nn.dynamic_rnn(\n",
    "            cells, embed_x,\n",
    "            initial_state=self.initial_state)\n",
    "        # Note: lstm_outputs shape:\n",
    "        ##  [batch_size, max_time, cells.output_size]\n",
    "        print('\\n  << lstm_output   >> ', lstm_outputs)\n",
    "        print('\\n  << final state   >> ', self.final_state)\n",
    "\n",
    "        # Apply a FC layer after on top of RNN output:\n",
    "        logits = tf.layers.dense(\n",
    "            inputs=lstm_outputs[:, -1],\n",
    "            units=1, activation=None,\n",
    "            name='logits')\n",
    "\n",
    "        logits = tf.squeeze(logits, name='logits_squeezed')\n",
    "        print('\\n  << logits        >> ', logits)\n",
    "\n",
    "        y_proba = tf.nn.sigmoid(logits, name='probabilities')\n",
    "        predictions = {\n",
    "            'probabilities': y_proba,\n",
    "            'labels': tf.cast(tf.round(y_proba), tf.int32,\n",
    "                              name='labels')\n",
    "        }\n",
    "        print('\\n  << predictions   >> ', predictions)\n",
    "\n",
    "        # Define the cost function\n",
    "        cost = tf.reduce_mean(\n",
    "            tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                labels=tf_y, logits=logits),\n",
    "            name='cost')\n",
    "\n",
    "        # Define the optimizer\n",
    "        optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "        train_op = optimizer.minimize(cost, name='train_op')\n",
    "\n",
    "    def train(self, X_train, y_train, num_epochs):\n",
    "        with tf.Session(graph=self.g) as sess:\n",
    "            sess.run(self.init_op)\n",
    "            iteration = 1\n",
    "            for epoch in range(num_epochs):\n",
    "                state = sess.run(self.initial_state)\n",
    "\n",
    "                for batch_x, batch_y in create_batch_generator(\n",
    "                        X_train, y_train, self.batch_size):\n",
    "                    feed = {'tf_x:0': batch_x,\n",
    "                            'tf_y:0': batch_y,\n",
    "                            'tf_keepprob:0': 0.5,\n",
    "                            self.initial_state: state}\n",
    "                    loss, _, state = sess.run(\n",
    "                        ['cost:0', 'train_op',\n",
    "                         self.final_state],\n",
    "                        feed_dict=feed)\n",
    "\n",
    "                    if iteration % 20 == 0:\n",
    "                        print(\"Epoch: %d/%d Iteration: %d \"\n",
    "                              \"| Train loss: %.5f\" % (\n",
    "                                  epoch + 1, num_epochs,\n",
    "                                  iteration, loss))\n",
    "\n",
    "                    iteration += 1\n",
    "                if (epoch+1) % 10 == 0:\n",
    "                    self.saver.save(sess,\n",
    "                                    \"model/sentiment-%d.ckpt\" % epoch)\n",
    "\n",
    "    def predict(self, X_data, return_proba=False):\n",
    "        preds = []\n",
    "        with tf.Session(graph=self.g) as sess:\n",
    "            self.saver.restore(\n",
    "                sess, tf.train.latest_checkpoint('model/'))\n",
    "            test_state = sess.run(self.initial_state)\n",
    "            for ii, batch_x in enumerate(\n",
    "                create_batch_generator(\n",
    "                    X_data, None, batch_size=self.batch_size), 1):\n",
    "                feed = {'tf_x:0': batch_x,\n",
    "                        'tf_keepprob:0': 1.0,\n",
    "                        self.initial_state: test_state}\n",
    "                if return_proba:\n",
    "                    pred, test_state = sess.run(\n",
    "                        ['probabilities:0', self.final_state],\n",
    "                        feed_dict=feed)\n",
    "                else:\n",
    "                    pred, test_state = sess.run(\n",
    "                        ['labels:0', self.final_state],\n",
    "                        feed_dict=feed)\n",
    "\n",
    "                preds.append(pred)\n",
    "\n",
    "        return np.concatenate(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实例化SentimentRNN类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  << initial state >>  (LSTMStateTuple(c=<tf.Tensor 'MultiRNNCellZeroState/DropoutWrapperZeroState/BasicLSTMCellZeroState/zeros:0' shape=(100, 128) dtype=float32>, h=<tf.Tensor 'MultiRNNCellZeroState/DropoutWrapperZeroState/BasicLSTMCellZeroState/zeros_1:0' shape=(100, 128) dtype=float32>),)\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x000001A044EBECF8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x000001A044EBECF8>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x000001A044EBECF8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x000001A044EBECF8>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x000001A044EBEBA8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x000001A044EBEBA8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x000001A044EBEBA8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x000001A044EBEBA8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "\n",
      "  << lstm_output   >>  Tensor(\"rnn/transpose_1:0\", shape=(100, 200, 128), dtype=float32)\n",
      "\n",
      "  << final state   >>  (LSTMStateTuple(c=<tf.Tensor 'rnn/while/Exit_3:0' shape=(100, 128) dtype=float32>, h=<tf.Tensor 'rnn/while/Exit_4:0' shape=(100, 128) dtype=float32>),)\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001A044F81B38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001A044F81B38>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001A044F81B38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001A044F81B38>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "\n",
      "  << logits        >>  Tensor(\"logits_squeezed:0\", shape=(100,), dtype=float32)\n",
      "\n",
      "  << predictions   >>  {'probabilities': <tf.Tensor 'probabilities:0' shape=(100,) dtype=float32>, 'labels': <tf.Tensor 'labels:0' shape=(100,) dtype=int32>}\n"
     ]
    }
   ],
   "source": [
    "n_words = max(list(word_to_int.values())) + 1\n",
    "rnn = SentimentRNN(n_words=n_words, seq_len=sequence_length, embed_size=256, lstm_size=128,\n",
    "                   num_layers=1, batch_size=100, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练和优化情感分析RNN模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/40 Iteration: 20 | Train loss: 0.68144\n",
      "Epoch: 1/40 Iteration: 40 | Train loss: 0.62962\n",
      "Epoch: 1/40 Iteration: 60 | Train loss: 0.64887\n",
      "Epoch: 1/40 Iteration: 80 | Train loss: 0.62561\n",
      "Epoch: 1/40 Iteration: 100 | Train loss: 0.57090\n",
      "Epoch: 1/40 Iteration: 120 | Train loss: 0.53748\n",
      "Epoch: 1/40 Iteration: 140 | Train loss: 0.47973\n",
      "Epoch: 1/40 Iteration: 160 | Train loss: 0.43372\n",
      "Epoch: 1/40 Iteration: 180 | Train loss: 0.47417\n",
      "Epoch: 1/40 Iteration: 200 | Train loss: 0.61672\n",
      "Epoch: 1/40 Iteration: 220 | Train loss: 0.34326\n",
      "Epoch: 1/40 Iteration: 240 | Train loss: 0.45520\n",
      "Epoch: 2/40 Iteration: 260 | Train loss: 0.41717\n",
      "Epoch: 2/40 Iteration: 280 | Train loss: 0.37721\n",
      "Epoch: 2/40 Iteration: 300 | Train loss: 0.32072\n",
      "Epoch: 2/40 Iteration: 320 | Train loss: 0.42077\n",
      "Epoch: 2/40 Iteration: 340 | Train loss: 0.32731\n",
      "Epoch: 2/40 Iteration: 360 | Train loss: 0.23789\n",
      "Epoch: 2/40 Iteration: 380 | Train loss: 0.36921\n",
      "Epoch: 2/40 Iteration: 400 | Train loss: 0.35867\n",
      "Epoch: 2/40 Iteration: 420 | Train loss: 0.30084\n",
      "Epoch: 2/40 Iteration: 440 | Train loss: 0.26023\n",
      "Epoch: 2/40 Iteration: 460 | Train loss: 0.47996\n",
      "Epoch: 2/40 Iteration: 480 | Train loss: 0.28514\n",
      "Epoch: 2/40 Iteration: 500 | Train loss: 0.22047\n",
      "Epoch: 3/40 Iteration: 520 | Train loss: 0.32712\n",
      "Epoch: 3/40 Iteration: 540 | Train loss: 0.26369\n",
      "Epoch: 3/40 Iteration: 560 | Train loss: 0.35068\n",
      "Epoch: 3/40 Iteration: 580 | Train loss: 0.27584\n",
      "Epoch: 3/40 Iteration: 600 | Train loss: 0.22144\n",
      "Epoch: 3/40 Iteration: 620 | Train loss: 0.30593\n",
      "Epoch: 3/40 Iteration: 640 | Train loss: 0.22242\n",
      "Epoch: 3/40 Iteration: 660 | Train loss: 0.18997\n",
      "Epoch: 3/40 Iteration: 680 | Train loss: 0.27962\n",
      "Epoch: 3/40 Iteration: 700 | Train loss: 0.23518\n",
      "Epoch: 3/40 Iteration: 720 | Train loss: 0.20891\n",
      "Epoch: 3/40 Iteration: 740 | Train loss: 0.27644\n",
      "Epoch: 4/40 Iteration: 760 | Train loss: 0.22448\n",
      "Epoch: 4/40 Iteration: 780 | Train loss: 0.22141\n",
      "Epoch: 4/40 Iteration: 800 | Train loss: 0.23432\n",
      "Epoch: 4/40 Iteration: 820 | Train loss: 0.25324\n",
      "Epoch: 4/40 Iteration: 840 | Train loss: 0.20872\n",
      "Epoch: 4/40 Iteration: 860 | Train loss: 0.11363\n",
      "Epoch: 4/40 Iteration: 880 | Train loss: 0.21307\n",
      "Epoch: 4/40 Iteration: 900 | Train loss: 0.16681\n",
      "Epoch: 4/40 Iteration: 920 | Train loss: 0.25209\n",
      "Epoch: 4/40 Iteration: 940 | Train loss: 0.17046\n",
      "Epoch: 4/40 Iteration: 960 | Train loss: 0.26864\n",
      "Epoch: 4/40 Iteration: 980 | Train loss: 0.18037\n",
      "Epoch: 4/40 Iteration: 1000 | Train loss: 0.15617\n",
      "Epoch: 5/40 Iteration: 1020 | Train loss: 0.16734\n",
      "Epoch: 5/40 Iteration: 1040 | Train loss: 0.12536\n",
      "Epoch: 5/40 Iteration: 1060 | Train loss: 0.19509\n",
      "Epoch: 5/40 Iteration: 1080 | Train loss: 0.13235\n",
      "Epoch: 5/40 Iteration: 1100 | Train loss: 0.14023\n",
      "Epoch: 5/40 Iteration: 1120 | Train loss: 0.16867\n",
      "Epoch: 5/40 Iteration: 1140 | Train loss: 0.07417\n",
      "Epoch: 5/40 Iteration: 1160 | Train loss: 0.10801\n",
      "Epoch: 5/40 Iteration: 1180 | Train loss: 0.24945\n",
      "Epoch: 5/40 Iteration: 1200 | Train loss: 0.15121\n",
      "Epoch: 5/40 Iteration: 1220 | Train loss: 0.04285\n",
      "Epoch: 5/40 Iteration: 1240 | Train loss: 0.12422\n",
      "Epoch: 6/40 Iteration: 1260 | Train loss: 0.04673\n",
      "Epoch: 6/40 Iteration: 1280 | Train loss: 0.02171\n",
      "Epoch: 6/40 Iteration: 1300 | Train loss: 0.14757\n",
      "Epoch: 6/40 Iteration: 1320 | Train loss: 0.14075\n",
      "Epoch: 6/40 Iteration: 1340 | Train loss: 0.03296\n",
      "Epoch: 6/40 Iteration: 1360 | Train loss: 0.07017\n",
      "Epoch: 6/40 Iteration: 1380 | Train loss: 0.22426\n",
      "Epoch: 6/40 Iteration: 1400 | Train loss: 0.16349\n",
      "Epoch: 6/40 Iteration: 1420 | Train loss: 0.23977\n",
      "Epoch: 6/40 Iteration: 1440 | Train loss: 0.12911\n",
      "Epoch: 6/40 Iteration: 1460 | Train loss: 0.26021\n",
      "Epoch: 6/40 Iteration: 1480 | Train loss: 0.10352\n",
      "Epoch: 6/40 Iteration: 1500 | Train loss: 0.07121\n",
      "Epoch: 7/40 Iteration: 1520 | Train loss: 0.13234\n",
      "Epoch: 7/40 Iteration: 1540 | Train loss: 0.01671\n",
      "Epoch: 7/40 Iteration: 1560 | Train loss: 0.11355\n",
      "Epoch: 7/40 Iteration: 1580 | Train loss: 0.07359\n",
      "Epoch: 7/40 Iteration: 1600 | Train loss: 0.04417\n",
      "Epoch: 7/40 Iteration: 1620 | Train loss: 0.13588\n",
      "Epoch: 7/40 Iteration: 1640 | Train loss: 0.07858\n",
      "Epoch: 7/40 Iteration: 1660 | Train loss: 0.11153\n",
      "Epoch: 7/40 Iteration: 1680 | Train loss: 0.15586\n",
      "Epoch: 7/40 Iteration: 1700 | Train loss: 0.07214\n",
      "Epoch: 7/40 Iteration: 1720 | Train loss: 0.09339\n",
      "Epoch: 7/40 Iteration: 1740 | Train loss: 0.06464\n",
      "Epoch: 8/40 Iteration: 1760 | Train loss: 0.01633\n",
      "Epoch: 8/40 Iteration: 1780 | Train loss: 0.01747\n",
      "Epoch: 8/40 Iteration: 1800 | Train loss: 0.03260\n",
      "Epoch: 8/40 Iteration: 1820 | Train loss: 0.04923\n",
      "Epoch: 8/40 Iteration: 1840 | Train loss: 0.02865\n",
      "Epoch: 8/40 Iteration: 1860 | Train loss: 0.05264\n",
      "Epoch: 8/40 Iteration: 1880 | Train loss: 0.24141\n",
      "Epoch: 8/40 Iteration: 1900 | Train loss: 0.08481\n",
      "Epoch: 8/40 Iteration: 1920 | Train loss: 0.09444\n",
      "Epoch: 8/40 Iteration: 1940 | Train loss: 0.08829\n",
      "Epoch: 8/40 Iteration: 1960 | Train loss: 0.01786\n",
      "Epoch: 8/40 Iteration: 1980 | Train loss: 0.09785\n",
      "Epoch: 8/40 Iteration: 2000 | Train loss: 0.05369\n",
      "Epoch: 9/40 Iteration: 2020 | Train loss: 0.09763\n",
      "Epoch: 9/40 Iteration: 2040 | Train loss: 0.01878\n",
      "Epoch: 9/40 Iteration: 2060 | Train loss: 0.07063\n",
      "Epoch: 9/40 Iteration: 2080 | Train loss: 0.03360\n",
      "Epoch: 9/40 Iteration: 2100 | Train loss: 0.06411\n",
      "Epoch: 9/40 Iteration: 2120 | Train loss: 0.14998\n",
      "Epoch: 9/40 Iteration: 2140 | Train loss: 0.01444\n",
      "Epoch: 9/40 Iteration: 2160 | Train loss: 0.03157\n",
      "Epoch: 9/40 Iteration: 2180 | Train loss: 0.01436\n",
      "Epoch: 9/40 Iteration: 2200 | Train loss: 0.04118\n",
      "Epoch: 9/40 Iteration: 2220 | Train loss: 0.15348\n",
      "Epoch: 9/40 Iteration: 2240 | Train loss: 0.14813\n",
      "Epoch: 10/40 Iteration: 2260 | Train loss: 0.03220\n",
      "Epoch: 10/40 Iteration: 2280 | Train loss: 0.04341\n",
      "Epoch: 10/40 Iteration: 2300 | Train loss: 0.01772\n",
      "Epoch: 10/40 Iteration: 2320 | Train loss: 0.05929\n",
      "Epoch: 10/40 Iteration: 2340 | Train loss: 0.06143\n",
      "Epoch: 10/40 Iteration: 2360 | Train loss: 0.04639\n",
      "Epoch: 10/40 Iteration: 2380 | Train loss: 0.13959\n",
      "Epoch: 10/40 Iteration: 2400 | Train loss: 0.04222\n",
      "Epoch: 10/40 Iteration: 2420 | Train loss: 0.03275\n",
      "Epoch: 10/40 Iteration: 2440 | Train loss: 0.01773\n",
      "Epoch: 10/40 Iteration: 2460 | Train loss: 0.27541\n",
      "Epoch: 10/40 Iteration: 2480 | Train loss: 0.04397\n",
      "Epoch: 10/40 Iteration: 2500 | Train loss: 0.09275\n",
      "Epoch: 11/40 Iteration: 2520 | Train loss: 0.09194\n",
      "Epoch: 11/40 Iteration: 2540 | Train loss: 0.00998\n",
      "Epoch: 11/40 Iteration: 2560 | Train loss: 0.04265\n",
      "Epoch: 11/40 Iteration: 2580 | Train loss: 0.05109\n",
      "Epoch: 11/40 Iteration: 2600 | Train loss: 0.04539\n",
      "Epoch: 11/40 Iteration: 2620 | Train loss: 0.06123\n",
      "Epoch: 11/40 Iteration: 2640 | Train loss: 0.00656\n",
      "Epoch: 11/40 Iteration: 2660 | Train loss: 0.00455\n",
      "Epoch: 11/40 Iteration: 2680 | Train loss: 0.05467\n",
      "Epoch: 11/40 Iteration: 2700 | Train loss: 0.01562\n",
      "Epoch: 11/40 Iteration: 2720 | Train loss: 0.00756\n",
      "Epoch: 11/40 Iteration: 2740 | Train loss: 0.04434\n",
      "Epoch: 12/40 Iteration: 2760 | Train loss: 0.06466\n",
      "Epoch: 12/40 Iteration: 2780 | Train loss: 0.00900\n",
      "Epoch: 12/40 Iteration: 2800 | Train loss: 0.00358\n",
      "Epoch: 12/40 Iteration: 2820 | Train loss: 0.00731\n",
      "Epoch: 12/40 Iteration: 2840 | Train loss: 0.00590\n",
      "Epoch: 12/40 Iteration: 2860 | Train loss: 0.00274\n",
      "Epoch: 12/40 Iteration: 2880 | Train loss: 0.06171\n",
      "Epoch: 12/40 Iteration: 2900 | Train loss: 0.01247\n",
      "Epoch: 12/40 Iteration: 2920 | Train loss: 0.02236\n",
      "Epoch: 12/40 Iteration: 2940 | Train loss: 0.00306\n",
      "Epoch: 12/40 Iteration: 2960 | Train loss: 0.02137\n",
      "Epoch: 12/40 Iteration: 2980 | Train loss: 0.00890\n",
      "Epoch: 12/40 Iteration: 3000 | Train loss: 0.00416\n",
      "Epoch: 13/40 Iteration: 3020 | Train loss: 0.00977\n",
      "Epoch: 13/40 Iteration: 3040 | Train loss: 0.00243\n",
      "Epoch: 13/40 Iteration: 3060 | Train loss: 0.00194\n",
      "Epoch: 13/40 Iteration: 3080 | Train loss: 0.00394\n",
      "Epoch: 13/40 Iteration: 3100 | Train loss: 0.00128\n",
      "Epoch: 13/40 Iteration: 3120 | Train loss: 0.04189\n",
      "Epoch: 13/40 Iteration: 3140 | Train loss: 0.00570\n",
      "Epoch: 13/40 Iteration: 3160 | Train loss: 0.00114\n",
      "Epoch: 13/40 Iteration: 3180 | Train loss: 0.00099\n",
      "Epoch: 13/40 Iteration: 3200 | Train loss: 0.00110\n",
      "Epoch: 13/40 Iteration: 3220 | Train loss: 0.00072\n",
      "Epoch: 13/40 Iteration: 3240 | Train loss: 0.00409\n",
      "Epoch: 14/40 Iteration: 3260 | Train loss: 0.00121\n",
      "Epoch: 14/40 Iteration: 3280 | Train loss: 0.00090\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14/40 Iteration: 3300 | Train loss: 0.00145\n",
      "Epoch: 14/40 Iteration: 3320 | Train loss: 0.00062\n",
      "Epoch: 14/40 Iteration: 3340 | Train loss: 0.00173\n",
      "Epoch: 14/40 Iteration: 3360 | Train loss: 0.00309\n",
      "Epoch: 14/40 Iteration: 3380 | Train loss: 0.03511\n",
      "Epoch: 14/40 Iteration: 3400 | Train loss: 0.00164\n",
      "Epoch: 14/40 Iteration: 3420 | Train loss: 0.00097\n",
      "Epoch: 14/40 Iteration: 3440 | Train loss: 0.00041\n",
      "Epoch: 14/40 Iteration: 3460 | Train loss: 0.00139\n",
      "Epoch: 14/40 Iteration: 3480 | Train loss: 0.00182\n",
      "Epoch: 14/40 Iteration: 3500 | Train loss: 0.00250\n",
      "Epoch: 15/40 Iteration: 3520 | Train loss: 0.00712\n",
      "Epoch: 15/40 Iteration: 3540 | Train loss: 0.00053\n",
      "Epoch: 15/40 Iteration: 3560 | Train loss: 0.00096\n",
      "Epoch: 15/40 Iteration: 3580 | Train loss: 0.01974\n",
      "Epoch: 15/40 Iteration: 3600 | Train loss: 0.00230\n",
      "Epoch: 15/40 Iteration: 3620 | Train loss: 0.01154\n",
      "Epoch: 15/40 Iteration: 3640 | Train loss: 0.00033\n",
      "Epoch: 15/40 Iteration: 3660 | Train loss: 0.02580\n",
      "Epoch: 15/40 Iteration: 3680 | Train loss: 0.00232\n",
      "Epoch: 15/40 Iteration: 3700 | Train loss: 0.00056\n",
      "Epoch: 15/40 Iteration: 3720 | Train loss: 0.00036\n",
      "Epoch: 15/40 Iteration: 3740 | Train loss: 0.00205\n",
      "Epoch: 16/40 Iteration: 3760 | Train loss: 0.00053\n",
      "Epoch: 16/40 Iteration: 3780 | Train loss: 0.00133\n",
      "Epoch: 16/40 Iteration: 3800 | Train loss: 0.00069\n",
      "Epoch: 16/40 Iteration: 3820 | Train loss: 0.01077\n",
      "Epoch: 16/40 Iteration: 3840 | Train loss: 0.00962\n",
      "Epoch: 16/40 Iteration: 3860 | Train loss: 0.05937\n",
      "Epoch: 16/40 Iteration: 3880 | Train loss: 0.00486\n",
      "Epoch: 16/40 Iteration: 3900 | Train loss: 0.00983\n",
      "Epoch: 16/40 Iteration: 3920 | Train loss: 0.00508\n",
      "Epoch: 16/40 Iteration: 3940 | Train loss: 0.00173\n",
      "Epoch: 16/40 Iteration: 3960 | Train loss: 0.00326\n",
      "Epoch: 16/40 Iteration: 3980 | Train loss: 0.00200\n",
      "Epoch: 16/40 Iteration: 4000 | Train loss: 0.00411\n",
      "Epoch: 17/40 Iteration: 4020 | Train loss: 0.00230\n",
      "Epoch: 17/40 Iteration: 4040 | Train loss: 0.00661\n",
      "Epoch: 17/40 Iteration: 4060 | Train loss: 0.00104\n",
      "Epoch: 17/40 Iteration: 4080 | Train loss: 0.01621\n",
      "Epoch: 17/40 Iteration: 4100 | Train loss: 0.00300\n",
      "Epoch: 17/40 Iteration: 4120 | Train loss: 0.02805\n",
      "Epoch: 17/40 Iteration: 4140 | Train loss: 0.00160\n",
      "Epoch: 17/40 Iteration: 4160 | Train loss: 0.00117\n",
      "Epoch: 17/40 Iteration: 4180 | Train loss: 0.03150\n",
      "Epoch: 17/40 Iteration: 4200 | Train loss: 0.00101\n",
      "Epoch: 17/40 Iteration: 4220 | Train loss: 0.00062\n",
      "Epoch: 17/40 Iteration: 4240 | Train loss: 0.00444\n",
      "Epoch: 18/40 Iteration: 4260 | Train loss: 0.00257\n",
      "Epoch: 18/40 Iteration: 4280 | Train loss: 0.01944\n",
      "Epoch: 18/40 Iteration: 4300 | Train loss: 0.00555\n",
      "Epoch: 18/40 Iteration: 4320 | Train loss: 0.00056\n",
      "Epoch: 18/40 Iteration: 4340 | Train loss: 0.00580\n",
      "Epoch: 18/40 Iteration: 4360 | Train loss: 0.00029\n",
      "Epoch: 18/40 Iteration: 4380 | Train loss: 0.00706\n",
      "Epoch: 18/40 Iteration: 4400 | Train loss: 0.00524\n",
      "Epoch: 18/40 Iteration: 4420 | Train loss: 0.00100\n",
      "Epoch: 18/40 Iteration: 4440 | Train loss: 0.00205\n",
      "Epoch: 18/40 Iteration: 4460 | Train loss: 0.02215\n",
      "Epoch: 18/40 Iteration: 4480 | Train loss: 0.01106\n",
      "Epoch: 18/40 Iteration: 4500 | Train loss: 0.01846\n",
      "Epoch: 19/40 Iteration: 4520 | Train loss: 0.00711\n",
      "Epoch: 19/40 Iteration: 4540 | Train loss: 0.00951\n",
      "Epoch: 19/40 Iteration: 4560 | Train loss: 0.00361\n",
      "Epoch: 19/40 Iteration: 4580 | Train loss: 0.03185\n",
      "Epoch: 19/40 Iteration: 4600 | Train loss: 0.12235\n",
      "Epoch: 19/40 Iteration: 4620 | Train loss: 0.09189\n",
      "Epoch: 19/40 Iteration: 4640 | Train loss: 0.02471\n",
      "Epoch: 19/40 Iteration: 4660 | Train loss: 0.01951\n",
      "Epoch: 19/40 Iteration: 4680 | Train loss: 0.03723\n",
      "Epoch: 19/40 Iteration: 4700 | Train loss: 0.03345\n",
      "Epoch: 19/40 Iteration: 4720 | Train loss: 0.00313\n",
      "Epoch: 19/40 Iteration: 4740 | Train loss: 0.02438\n",
      "Epoch: 20/40 Iteration: 4760 | Train loss: 0.00456\n",
      "Epoch: 20/40 Iteration: 4780 | Train loss: 0.05201\n",
      "Epoch: 20/40 Iteration: 4800 | Train loss: 0.00758\n",
      "Epoch: 20/40 Iteration: 4820 | Train loss: 0.02645\n",
      "Epoch: 20/40 Iteration: 4840 | Train loss: 0.10207\n",
      "Epoch: 20/40 Iteration: 4860 | Train loss: 0.01199\n",
      "Epoch: 20/40 Iteration: 4880 | Train loss: 0.01873\n",
      "Epoch: 20/40 Iteration: 4900 | Train loss: 0.01773\n",
      "Epoch: 20/40 Iteration: 4920 | Train loss: 0.01287\n",
      "Epoch: 20/40 Iteration: 4940 | Train loss: 0.01006\n",
      "Epoch: 20/40 Iteration: 4960 | Train loss: 0.00702\n",
      "Epoch: 20/40 Iteration: 4980 | Train loss: 0.00171\n",
      "Epoch: 20/40 Iteration: 5000 | Train loss: 0.01588\n",
      "Epoch: 21/40 Iteration: 5020 | Train loss: 0.07651\n",
      "Epoch: 21/40 Iteration: 5040 | Train loss: 0.00283\n",
      "Epoch: 21/40 Iteration: 5060 | Train loss: 0.02761\n",
      "Epoch: 21/40 Iteration: 5080 | Train loss: 0.00971\n",
      "Epoch: 21/40 Iteration: 5100 | Train loss: 0.00587\n",
      "Epoch: 21/40 Iteration: 5120 | Train loss: 0.00407\n",
      "Epoch: 21/40 Iteration: 5140 | Train loss: 0.00182\n",
      "Epoch: 21/40 Iteration: 5160 | Train loss: 0.00176\n",
      "Epoch: 21/40 Iteration: 5180 | Train loss: 0.00366\n",
      "Epoch: 21/40 Iteration: 5200 | Train loss: 0.00086\n",
      "Epoch: 21/40 Iteration: 5220 | Train loss: 0.00155\n",
      "Epoch: 21/40 Iteration: 5240 | Train loss: 0.00357\n",
      "Epoch: 22/40 Iteration: 5260 | Train loss: 0.00074\n",
      "Epoch: 22/40 Iteration: 5280 | Train loss: 0.00074\n",
      "Epoch: 22/40 Iteration: 5300 | Train loss: 0.00203\n",
      "Epoch: 22/40 Iteration: 5320 | Train loss: 0.00098\n",
      "Epoch: 22/40 Iteration: 5340 | Train loss: 0.00076\n",
      "Epoch: 22/40 Iteration: 5360 | Train loss: 0.00055\n",
      "Epoch: 22/40 Iteration: 5380 | Train loss: 0.00131\n",
      "Epoch: 22/40 Iteration: 5400 | Train loss: 0.00062\n",
      "Epoch: 22/40 Iteration: 5420 | Train loss: 0.00046\n",
      "Epoch: 22/40 Iteration: 5440 | Train loss: 0.00051\n",
      "Epoch: 22/40 Iteration: 5460 | Train loss: 0.00028\n",
      "Epoch: 22/40 Iteration: 5480 | Train loss: 0.00260\n",
      "Epoch: 22/40 Iteration: 5500 | Train loss: 0.00029\n",
      "Epoch: 23/40 Iteration: 5520 | Train loss: 0.00326\n",
      "Epoch: 23/40 Iteration: 5540 | Train loss: 0.00012\n",
      "Epoch: 23/40 Iteration: 5560 | Train loss: 0.00067\n",
      "Epoch: 23/40 Iteration: 5580 | Train loss: 0.00635\n",
      "Epoch: 23/40 Iteration: 5600 | Train loss: 0.00099\n",
      "Epoch: 23/40 Iteration: 5620 | Train loss: 0.00050\n",
      "Epoch: 23/40 Iteration: 5640 | Train loss: 0.00010\n",
      "Epoch: 23/40 Iteration: 5660 | Train loss: 0.00081\n",
      "Epoch: 23/40 Iteration: 5680 | Train loss: 0.00309\n",
      "Epoch: 23/40 Iteration: 5700 | Train loss: 0.00014\n",
      "Epoch: 23/40 Iteration: 5720 | Train loss: 0.00017\n",
      "Epoch: 23/40 Iteration: 5740 | Train loss: 0.00105\n",
      "Epoch: 24/40 Iteration: 5760 | Train loss: 0.00014\n",
      "Epoch: 24/40 Iteration: 5780 | Train loss: 0.00022\n",
      "Epoch: 24/40 Iteration: 5800 | Train loss: 0.00024\n",
      "Epoch: 24/40 Iteration: 5820 | Train loss: 0.00017\n",
      "Epoch: 24/40 Iteration: 5840 | Train loss: 0.00051\n",
      "Epoch: 24/40 Iteration: 5860 | Train loss: 0.00009\n",
      "Epoch: 24/40 Iteration: 5880 | Train loss: 0.00062\n",
      "Epoch: 24/40 Iteration: 5900 | Train loss: 0.00048\n",
      "Epoch: 24/40 Iteration: 5920 | Train loss: 0.00022\n",
      "Epoch: 24/40 Iteration: 5940 | Train loss: 0.00024\n",
      "Epoch: 24/40 Iteration: 5960 | Train loss: 0.00011\n",
      "Epoch: 24/40 Iteration: 5980 | Train loss: 0.00041\n",
      "Epoch: 24/40 Iteration: 6000 | Train loss: 0.00017\n",
      "Epoch: 25/40 Iteration: 6020 | Train loss: 0.00054\n",
      "Epoch: 25/40 Iteration: 6040 | Train loss: 0.00007\n",
      "Epoch: 25/40 Iteration: 6060 | Train loss: 0.00026\n",
      "Epoch: 25/40 Iteration: 6080 | Train loss: 0.00022\n",
      "Epoch: 25/40 Iteration: 6100 | Train loss: 0.00036\n",
      "Epoch: 25/40 Iteration: 6120 | Train loss: 0.00037\n",
      "Epoch: 25/40 Iteration: 6140 | Train loss: 0.00006\n",
      "Epoch: 25/40 Iteration: 6160 | Train loss: 0.00065\n",
      "Epoch: 25/40 Iteration: 6180 | Train loss: 0.00017\n",
      "Epoch: 25/40 Iteration: 6200 | Train loss: 0.00018\n",
      "Epoch: 25/40 Iteration: 6220 | Train loss: 0.00015\n",
      "Epoch: 25/40 Iteration: 6240 | Train loss: 0.00032\n",
      "Epoch: 26/40 Iteration: 6260 | Train loss: 0.00006\n",
      "Epoch: 26/40 Iteration: 6280 | Train loss: 0.00006\n",
      "Epoch: 26/40 Iteration: 6300 | Train loss: 0.00034\n",
      "Epoch: 26/40 Iteration: 6320 | Train loss: 0.00011\n",
      "Epoch: 26/40 Iteration: 6340 | Train loss: 0.00037\n",
      "Epoch: 26/40 Iteration: 6360 | Train loss: 0.00018\n",
      "Epoch: 26/40 Iteration: 6380 | Train loss: 0.00011\n",
      "Epoch: 26/40 Iteration: 6400 | Train loss: 0.00014\n",
      "Epoch: 26/40 Iteration: 6420 | Train loss: 0.00006\n",
      "Epoch: 26/40 Iteration: 6440 | Train loss: 0.00014\n",
      "Epoch: 26/40 Iteration: 6460 | Train loss: 0.00005\n",
      "Epoch: 26/40 Iteration: 6480 | Train loss: 0.00004\n",
      "Epoch: 26/40 Iteration: 6500 | Train loss: 0.00006\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 27/40 Iteration: 6520 | Train loss: 0.00016\n",
      "Epoch: 27/40 Iteration: 6540 | Train loss: 0.00002\n",
      "Epoch: 27/40 Iteration: 6560 | Train loss: 0.00008\n",
      "Epoch: 27/40 Iteration: 6580 | Train loss: 0.00021\n",
      "Epoch: 27/40 Iteration: 6600 | Train loss: 0.00007\n",
      "Epoch: 27/40 Iteration: 6620 | Train loss: 0.00012\n",
      "Epoch: 27/40 Iteration: 6640 | Train loss: 0.00005\n",
      "Epoch: 27/40 Iteration: 6660 | Train loss: 0.00037\n",
      "Epoch: 27/40 Iteration: 6680 | Train loss: 0.00004\n",
      "Epoch: 27/40 Iteration: 6700 | Train loss: 0.00004\n",
      "Epoch: 27/40 Iteration: 6720 | Train loss: 0.00006\n",
      "Epoch: 27/40 Iteration: 6740 | Train loss: 0.00010\n",
      "Epoch: 28/40 Iteration: 6760 | Train loss: 0.00005\n",
      "Epoch: 28/40 Iteration: 6780 | Train loss: 0.00006\n",
      "Epoch: 28/40 Iteration: 6800 | Train loss: 0.00010\n",
      "Epoch: 28/40 Iteration: 6820 | Train loss: 0.00002\n",
      "Epoch: 28/40 Iteration: 6840 | Train loss: 0.00018\n",
      "Epoch: 28/40 Iteration: 6860 | Train loss: 0.00005\n",
      "Epoch: 28/40 Iteration: 6880 | Train loss: 0.00063\n",
      "Epoch: 28/40 Iteration: 6900 | Train loss: 0.00011\n",
      "Epoch: 28/40 Iteration: 6920 | Train loss: 0.00076\n",
      "Epoch: 28/40 Iteration: 6940 | Train loss: 0.00006\n",
      "Epoch: 28/40 Iteration: 6960 | Train loss: 0.00004\n",
      "Epoch: 28/40 Iteration: 6980 | Train loss: 0.00007\n",
      "Epoch: 28/40 Iteration: 7000 | Train loss: 0.00005\n",
      "Epoch: 29/40 Iteration: 7020 | Train loss: 0.00014\n",
      "Epoch: 29/40 Iteration: 7040 | Train loss: 0.00002\n",
      "Epoch: 29/40 Iteration: 7060 | Train loss: 0.00009\n",
      "Epoch: 29/40 Iteration: 7080 | Train loss: 0.00006\n",
      "Epoch: 29/40 Iteration: 7100 | Train loss: 0.00011\n",
      "Epoch: 29/40 Iteration: 7120 | Train loss: 0.00009\n",
      "Epoch: 29/40 Iteration: 7140 | Train loss: 0.00001\n",
      "Epoch: 29/40 Iteration: 7160 | Train loss: 0.00004\n",
      "Epoch: 29/40 Iteration: 7180 | Train loss: 0.00002\n",
      "Epoch: 29/40 Iteration: 7200 | Train loss: 0.00001\n",
      "Epoch: 29/40 Iteration: 7220 | Train loss: 0.00002\n",
      "Epoch: 29/40 Iteration: 7240 | Train loss: 0.00067\n",
      "Epoch: 30/40 Iteration: 7260 | Train loss: 0.00003\n",
      "Epoch: 30/40 Iteration: 7280 | Train loss: 0.00004\n",
      "Epoch: 30/40 Iteration: 7300 | Train loss: 0.00004\n",
      "Epoch: 30/40 Iteration: 7320 | Train loss: 0.00004\n",
      "Epoch: 30/40 Iteration: 7340 | Train loss: 0.00092\n",
      "Epoch: 30/40 Iteration: 7360 | Train loss: 0.00006\n",
      "Epoch: 30/40 Iteration: 7380 | Train loss: 0.00006\n",
      "Epoch: 30/40 Iteration: 7400 | Train loss: 0.00010\n",
      "Epoch: 30/40 Iteration: 7420 | Train loss: 0.00002\n",
      "Epoch: 30/40 Iteration: 7440 | Train loss: 0.00003\n",
      "Epoch: 30/40 Iteration: 7460 | Train loss: 0.00002\n",
      "Epoch: 30/40 Iteration: 7480 | Train loss: 0.00007\n",
      "Epoch: 30/40 Iteration: 7500 | Train loss: 0.00002\n",
      "Epoch: 31/40 Iteration: 7520 | Train loss: 0.00013\n",
      "Epoch: 31/40 Iteration: 7540 | Train loss: 0.00001\n",
      "Epoch: 31/40 Iteration: 7560 | Train loss: 0.00002\n",
      "Epoch: 31/40 Iteration: 7580 | Train loss: 0.00002\n",
      "Epoch: 31/40 Iteration: 7600 | Train loss: 0.00001\n",
      "Epoch: 31/40 Iteration: 7620 | Train loss: 0.00003\n",
      "Epoch: 31/40 Iteration: 7640 | Train loss: 0.00004\n",
      "Epoch: 31/40 Iteration: 7660 | Train loss: 0.00003\n",
      "Epoch: 31/40 Iteration: 7680 | Train loss: 0.00006\n",
      "Epoch: 31/40 Iteration: 7700 | Train loss: 0.00001\n",
      "Epoch: 31/40 Iteration: 7720 | Train loss: 0.00002\n",
      "Epoch: 31/40 Iteration: 7740 | Train loss: 0.00002\n",
      "Epoch: 32/40 Iteration: 7760 | Train loss: 0.00006\n",
      "Epoch: 32/40 Iteration: 7780 | Train loss: 0.00004\n",
      "Epoch: 32/40 Iteration: 7800 | Train loss: 0.00004\n",
      "Epoch: 32/40 Iteration: 7820 | Train loss: 0.00001\n",
      "Epoch: 32/40 Iteration: 7840 | Train loss: 0.00009\n",
      "Epoch: 32/40 Iteration: 7860 | Train loss: 0.00005\n",
      "Epoch: 32/40 Iteration: 7880 | Train loss: 0.00005\n",
      "Epoch: 32/40 Iteration: 7900 | Train loss: 0.00004\n",
      "Epoch: 32/40 Iteration: 7920 | Train loss: 0.00013\n",
      "Epoch: 32/40 Iteration: 7940 | Train loss: 0.00002\n",
      "Epoch: 32/40 Iteration: 7960 | Train loss: 0.00001\n",
      "Epoch: 32/40 Iteration: 7980 | Train loss: 0.00005\n",
      "Epoch: 32/40 Iteration: 8000 | Train loss: 0.00001\n",
      "Epoch: 33/40 Iteration: 8020 | Train loss: 0.00002\n",
      "Epoch: 33/40 Iteration: 8040 | Train loss: 0.00001\n",
      "Epoch: 33/40 Iteration: 8060 | Train loss: 0.00003\n",
      "Epoch: 33/40 Iteration: 8080 | Train loss: 0.00005\n",
      "Epoch: 33/40 Iteration: 8100 | Train loss: 0.00001\n",
      "Epoch: 33/40 Iteration: 8120 | Train loss: 0.00004\n",
      "Epoch: 33/40 Iteration: 8140 | Train loss: 0.00001\n",
      "Epoch: 33/40 Iteration: 8160 | Train loss: 0.00002\n",
      "Epoch: 33/40 Iteration: 8180 | Train loss: 0.00002\n",
      "Epoch: 33/40 Iteration: 8200 | Train loss: 0.00000\n",
      "Epoch: 33/40 Iteration: 8220 | Train loss: 0.00001\n",
      "Epoch: 33/40 Iteration: 8240 | Train loss: 0.00001\n",
      "Epoch: 34/40 Iteration: 8260 | Train loss: 0.00002\n",
      "Epoch: 34/40 Iteration: 8280 | Train loss: 0.00002\n",
      "Epoch: 34/40 Iteration: 8300 | Train loss: 0.00001\n",
      "Epoch: 34/40 Iteration: 8320 | Train loss: 0.00000\n",
      "Epoch: 34/40 Iteration: 8340 | Train loss: 0.00012\n",
      "Epoch: 34/40 Iteration: 8360 | Train loss: 0.00001\n",
      "Epoch: 34/40 Iteration: 8380 | Train loss: 0.00002\n",
      "Epoch: 34/40 Iteration: 8400 | Train loss: 0.00007\n",
      "Epoch: 34/40 Iteration: 8420 | Train loss: 0.00001\n",
      "Epoch: 34/40 Iteration: 8440 | Train loss: 0.00006\n",
      "Epoch: 34/40 Iteration: 8460 | Train loss: 0.00002\n",
      "Epoch: 34/40 Iteration: 8480 | Train loss: 0.00002\n",
      "Epoch: 34/40 Iteration: 8500 | Train loss: 0.00001\n",
      "Epoch: 35/40 Iteration: 8520 | Train loss: 0.00004\n",
      "Epoch: 35/40 Iteration: 8540 | Train loss: 0.00001\n",
      "Epoch: 35/40 Iteration: 8560 | Train loss: 0.00002\n",
      "Epoch: 35/40 Iteration: 8580 | Train loss: 0.00001\n",
      "Epoch: 35/40 Iteration: 8600 | Train loss: 0.00001\n",
      "Epoch: 35/40 Iteration: 8620 | Train loss: 0.00001\n",
      "Epoch: 35/40 Iteration: 8640 | Train loss: 0.00002\n",
      "Epoch: 35/40 Iteration: 8660 | Train loss: 0.00001\n",
      "Epoch: 35/40 Iteration: 8680 | Train loss: 0.00001\n",
      "Epoch: 35/40 Iteration: 8700 | Train loss: 0.00000\n",
      "Epoch: 35/40 Iteration: 8720 | Train loss: 0.00000\n",
      "Epoch: 35/40 Iteration: 8740 | Train loss: 0.00001\n",
      "Epoch: 36/40 Iteration: 8760 | Train loss: 0.00000\n",
      "Epoch: 36/40 Iteration: 8780 | Train loss: 0.00002\n",
      "Epoch: 36/40 Iteration: 8800 | Train loss: 0.00001\n",
      "Epoch: 36/40 Iteration: 8820 | Train loss: 0.00000\n",
      "Epoch: 36/40 Iteration: 8840 | Train loss: 0.00001\n",
      "Epoch: 36/40 Iteration: 8860 | Train loss: 0.00001\n",
      "Epoch: 36/40 Iteration: 8880 | Train loss: 0.00000\n",
      "Epoch: 36/40 Iteration: 8900 | Train loss: 0.00004\n",
      "Epoch: 36/40 Iteration: 8920 | Train loss: 0.00000\n",
      "Epoch: 36/40 Iteration: 8940 | Train loss: 0.00003\n",
      "Epoch: 36/40 Iteration: 8960 | Train loss: 0.00001\n",
      "Epoch: 36/40 Iteration: 8980 | Train loss: 0.00004\n",
      "Epoch: 36/40 Iteration: 9000 | Train loss: 0.00000\n",
      "Epoch: 37/40 Iteration: 9020 | Train loss: 0.00004\n",
      "Epoch: 37/40 Iteration: 9040 | Train loss: 0.00002\n",
      "Epoch: 37/40 Iteration: 9060 | Train loss: 0.00001\n",
      "Epoch: 37/40 Iteration: 9080 | Train loss: 0.00002\n",
      "Epoch: 37/40 Iteration: 9100 | Train loss: 0.00001\n",
      "Epoch: 37/40 Iteration: 9120 | Train loss: 0.00001\n",
      "Epoch: 37/40 Iteration: 9140 | Train loss: 0.00001\n",
      "Epoch: 37/40 Iteration: 9160 | Train loss: 0.00001\n",
      "Epoch: 37/40 Iteration: 9180 | Train loss: 0.00001\n",
      "Epoch: 37/40 Iteration: 9200 | Train loss: 0.00000\n",
      "Epoch: 37/40 Iteration: 9220 | Train loss: 0.00004\n",
      "Epoch: 37/40 Iteration: 9240 | Train loss: 0.00001\n",
      "Epoch: 38/40 Iteration: 9260 | Train loss: 0.00001\n",
      "Epoch: 38/40 Iteration: 9280 | Train loss: 0.00001\n",
      "Epoch: 38/40 Iteration: 9300 | Train loss: 0.00001\n",
      "Epoch: 38/40 Iteration: 9320 | Train loss: 0.00001\n",
      "Epoch: 38/40 Iteration: 9340 | Train loss: 0.00001\n",
      "Epoch: 38/40 Iteration: 9360 | Train loss: 0.00000\n",
      "Epoch: 38/40 Iteration: 9380 | Train loss: 0.00001\n",
      "Epoch: 38/40 Iteration: 9400 | Train loss: 0.00001\n",
      "Epoch: 38/40 Iteration: 9420 | Train loss: 0.02450\n",
      "Epoch: 38/40 Iteration: 9440 | Train loss: 0.04184\n",
      "Epoch: 38/40 Iteration: 9460 | Train loss: 0.01631\n",
      "Epoch: 38/40 Iteration: 9480 | Train loss: 0.01109\n",
      "Epoch: 38/40 Iteration: 9500 | Train loss: 0.06423\n",
      "Epoch: 39/40 Iteration: 9520 | Train loss: 0.01886\n",
      "Epoch: 39/40 Iteration: 9540 | Train loss: 0.02467\n",
      "Epoch: 39/40 Iteration: 9560 | Train loss: 0.05632\n",
      "Epoch: 39/40 Iteration: 9580 | Train loss: 0.03763\n",
      "Epoch: 39/40 Iteration: 9600 | Train loss: 0.01477\n",
      "Epoch: 39/40 Iteration: 9620 | Train loss: 0.03638\n",
      "Epoch: 39/40 Iteration: 9640 | Train loss: 0.02146\n",
      "Epoch: 39/40 Iteration: 9660 | Train loss: 0.04324\n",
      "Epoch: 39/40 Iteration: 9680 | Train loss: 0.00073\n",
      "Epoch: 39/40 Iteration: 9700 | Train loss: 0.00620\n",
      "Epoch: 39/40 Iteration: 9720 | Train loss: 0.00149\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 39/40 Iteration: 9740 | Train loss: 0.00189\n",
      "Epoch: 40/40 Iteration: 9760 | Train loss: 0.00707\n",
      "Epoch: 40/40 Iteration: 9780 | Train loss: 0.06127\n",
      "Epoch: 40/40 Iteration: 9800 | Train loss: 0.00416\n",
      "Epoch: 40/40 Iteration: 9820 | Train loss: 0.00063\n",
      "Epoch: 40/40 Iteration: 9840 | Train loss: 0.00288\n",
      "Epoch: 40/40 Iteration: 9860 | Train loss: 0.00260\n",
      "Epoch: 40/40 Iteration: 9880 | Train loss: 0.08514\n",
      "Epoch: 40/40 Iteration: 9900 | Train loss: 0.00508\n",
      "Epoch: 40/40 Iteration: 9920 | Train loss: 0.00743\n",
      "Epoch: 40/40 Iteration: 9940 | Train loss: 0.00075\n",
      "Epoch: 40/40 Iteration: 9960 | Train loss: 0.00038\n",
      "Epoch: 40/40 Iteration: 9980 | Train loss: 0.00307\n",
      "Epoch: 40/40 Iteration: 10000 | Train loss: 0.00121\n"
     ]
    }
   ],
   "source": [
    "rnn.train(X_train, y_train, num_epochs=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Software\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from model/sentiment-39.ckpt\n",
      "Test Acc.: 0.857\n"
     ]
    }
   ],
   "source": [
    "preds = rnn.predict(X_test)\n",
    "y_true = y_test[:len(preds)]\n",
    "print('Test Acc.: {:.3f}'.format(np.sum(preds==y_true)/len(y_true)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from model/sentiment-39.ckpt\n",
      "[1.7583370e-06 9.9999797e-01 3.8330173e-01 ... 3.4272671e-06 3.3229589e-04\n",
      " 9.9958938e-01]\n"
     ]
    }
   ],
   "source": [
    "proba = rnn.predict(X_test, return_proba=True)\n",
    "print(proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
